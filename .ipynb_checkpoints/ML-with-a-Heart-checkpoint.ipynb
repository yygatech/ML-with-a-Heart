{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# set url of dataset\n",
    "values_url = \"./dataset/Warm_Up_Machine_Learning_with_a_Heart_-_Train_Values.csv\"\n",
    "labels_url = \"./dataset/Warm_Up_Machine_Learning_with_a_Heart_-_Train_Labels.csv\"\n",
    "test_values_url = \"./dataset/Warm_Up_Machine_Learning_with_a_Heart_-_Test_Values.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed: \n",
      "[[0.         0.00265201 0.         ... 0.11934031 0.45084116 0.        ]\n",
      " [0.         0.00341409 0.         ... 0.18436095 0.53942649 0.        ]\n",
      " [0.         0.00267026 0.         ... 0.20560986 0.43258178 0.00267026]\n",
      " ...\n",
      " [0.         0.         0.00274843 ... 0.17589971 0.36004471 0.00274843]\n",
      " [0.         0.00297026 0.         ... 0.14257265 0.51979612 0.        ]\n",
      " [0.         0.00323596 0.         ... 0.17474173 0.52746114 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "values_df = pd.read_csv(train_values_url)\n",
    "labels_df = pd.read_csv(train_labels_url)\n",
    "test_df = pd.read_csv(test_values_url)\n",
    "\n",
    "# concatenate values and labels\n",
    "# https://pandas.pydata.org/pandas-docs/stable/merging.html\n",
    "dataset_df = pd.merge(values_df, labels_df, on=['patient_id', 'patient_id'])\n",
    "\n",
    "# drop null value\n",
    "# https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html\n",
    "dataset_df.dropna()\n",
    "# Actually, nothing was dropped here.\n",
    "\n",
    "# convert to ndarray\n",
    "dataset_ndarr = dataset_df.values\n",
    "testset_ndarr = test_df.values\n",
    "\n",
    "# separate into values and labels\n",
    "ids, train = np.split(dataset_ndarr, [1], axis=1)\n",
    "values, labels = np.split(train, [-1], axis=1)\n",
    "test_ids, test_values = np.split(testset_ndarr, [1], axis=1)\n",
    "\n",
    "# one hot encoder for categoricalize\n",
    "# https://stackoverflow.com/questions/43588679/issue-with-onehotencoder-for-categorical-features\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, Normalizer\n",
    "ct = ColumnTransformer(\n",
    "    [('enc', OneHotEncoder(), [1])],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "# encoded = ct.fit_transform(values)\n",
    "\n",
    "# normalizer\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html#sklearn.preprocessing.Normalizer\n",
    "norm = Normalizer()\n",
    "# normalized = norm.fit_transform(encoded)\n",
    "\n",
    "# preprocessing pipeline\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "from sklearn.pipeline import Pipeline\n",
    "ppl = Pipeline(\n",
    "    [('ct', ct), \n",
    "     ('norm', norm)]\n",
    ")\n",
    "preprocessed = ppl.fit_transform(values)\n",
    "\n",
    "# flatten labels\n",
    "labels_list = labels.flatten().tolist()\n",
    "\n",
    "print(\"preprocessed: \")\n",
    "print(preprocessed)\n",
    "# print(len(preprocessed))\n",
    "# print()\n",
    "\n",
    "# print(\"labels_list: \")\n",
    "# print(labels_list)\n",
    "# print(len(labels_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "{'max_depth': 2, 'max_leaf_nodes': 6, 'min_samples_leaf': 1, 'min_samples_split': 2}\n"
     ]
    }
   ],
   "source": [
    "# classifiers\n",
    "from sklearn.tree import DecisionTreeRegressor as DT\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor as NN\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression\n",
    "models = {\n",
    "    'DT': DT(),\n",
    "    'NN': NN(),\n",
    "    'LR': LR()\n",
    "}\n",
    "param_dict = {\n",
    "    'DT': {\n",
    "        'max_depth': [1,2,3,None],\n",
    "        'max_leaf_nodes': [4,6,8,10,None],\n",
    "        'min_samples_leaf': [1,2,3],\n",
    "        'min_samples_split': [2,4,6]\n",
    "    },\n",
    "    'NN': {\n",
    "        'hidden_layer_sizes': [1,3,5],\n",
    "        'activation': ['logistic','tanh','relu'],\n",
    "        'early_stopping': [True, False],\n",
    "        'max_iter': [500,1000]\n",
    "    },\n",
    "    'LR': {\n",
    "        'penalty': ['l2'],\n",
    "        'solver': ['newton-cg','lbfgs','liblinear','sag','saga'],\n",
    "        'max_iter': [100,200,500,1000]\n",
    "    }\n",
    "}\n",
    "\n",
    "# grid search cross validation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "model_name = 'DT'\n",
    "model = models[model_name]\n",
    "gscv = GridSearchCV(model, param_dict[model_name], \n",
    "                    cv=5,scoring='neg_mean_squared_error')\n",
    "gscv.fit(preprocessed, labels_list)\n",
    "# gscv.predict(preprocessed)\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print(gscv.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.00266674 ... 0.15733779 0.424012   0.        ]\n",
      " [0.         0.00339223 0.         ... 0.11872803 0.61738575 0.        ]\n",
      " [0.         0.         0.00401573 ... 0.17267649 0.48188788 0.00401573]\n",
      " ...\n",
      " [0.         0.00295011 0.         ... 0.12390478 0.52512026 0.        ]\n",
      " [0.         0.00311077 0.         ... 0.14309541 0.47283701 0.00311077]\n",
      " [0.         0.00295372 0.         ... 0.12700996 0.50508612 0.        ]]\n",
      "[0.91304348 0.09090909 0.91304348 0.09090909 0.86666667 0.09090909\n",
      " 0.09090909 0.91304348 0.09090909 0.09090909 0.09090909 0.91304348\n",
      " 0.09090909 0.91304348 0.09090909 0.09090909 0.09090909 0.09090909\n",
      " 0.91304348 0.09090909 0.91304348 0.09090909 0.26666667 0.09090909\n",
      " 0.125      0.86666667 0.91304348 0.26666667 0.86666667 0.09090909\n",
      " 0.91304348 0.91304348 0.86666667 0.09090909 0.09090909 0.09090909\n",
      " 0.09090909 0.09090909 0.125      0.09090909 0.91304348 0.09090909\n",
      " 0.91304348 0.09090909 0.125      0.09090909 0.09090909 0.09090909\n",
      " 0.09090909 0.86666667 0.09090909 0.09090909 0.91304348 0.09090909\n",
      " 0.68421053 0.09090909 0.91304348 0.26666667 0.09090909 0.09090909\n",
      " 0.09090909 0.91304348 0.09090909 0.86666667 0.09090909 0.86666667\n",
      " 0.86666667 0.125      0.91304348 0.86666667 0.09090909 0.91304348\n",
      " 0.91304348 0.91304348 0.91304348 0.91304348 0.91304348 0.26666667\n",
      " 0.86666667 0.91304348 0.91304348 0.26666667 0.09090909 0.125\n",
      " 0.09090909 0.09090909 0.09090909 0.26666667 0.09090909 0.09090909]\n"
     ]
    }
   ],
   "source": [
    "# test model\n",
    "processed_test_values = ppl.transform(test_values)\n",
    "\n",
    "print(processed_train_values)\n",
    "\n",
    "test_pred = gscv.predict(processed_test_values)\n",
    "\n",
    "print(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to csv\n",
    "header = labels_df.columns\n",
    "col_id_name = header[0]\n",
    "col_label_name = header[1]\n",
    "\n",
    "test_ids_list = list(test_ids.flatten())\n",
    "\n",
    "res_dict = {col_id_name: test_ids_list,\n",
    "            col_label_name: test_pred\n",
    "           }\n",
    "res_df = pd.DataFrame.from_dict(res_dict)\n",
    "\n",
    "# export to df\n",
    "res_df.to_csv('./output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
