{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# set url of dataset\n",
    "train_values_url = \"../dataset/Warm_Up_Machine_Learning_with_a_Heart_-_Train_Values.csv\"\n",
    "train_labels_url = \"../dataset/Warm_Up_Machine_Learning_with_a_Heart_-_Train_Labels.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load dataset\n",
    "values_df = pd.read_csv(train_values_url)\n",
    "labels_df = pd.read_csv(train_labels_url)\n",
    "\n",
    "# concatenate values and labels\n",
    "# https://pandas.pydata.org/pandas-docs/stable/merging.html\n",
    "# dataset_df = pd.merge(values_df, labels_df, on=['patient_id', 'patient_id'])\n",
    "\n",
    "# convert to nd-array\n",
    "# dataset_nd = dataset_df.values\n",
    "\n",
    "# print(dataset_nd)\n",
    "# print(len(dataset_nd))\n",
    "# print(len(dataset_nd[0]))\n",
    "\n",
    "values_nd = values_df.values\n",
    "labels_nd = labels_df.values\n",
    "\n",
    "# optional\n",
    "# drop null value\n",
    "# https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html\n",
    "# dataset_df.dropna()\n",
    "# Actually, nothing was dropped here.\n",
    "\n",
    "# dataset_ndarr = dataset_df.values\n",
    "ids_nd = values_nd[:, 0]\n",
    "X_nd = values_nd[:, 1:]\n",
    "# ids_nd, X_nd = np.split(values_df.values, [1], axis=1)\n",
    "y_nd = labels_nd[:, -1]\n",
    "\n",
    "# print(ids_nd)\n",
    "# print(type(ids_nd))\n",
    "\n",
    "# print(X_nd)\n",
    "# print(type(X_nd))\n",
    "\n",
    "# print(y_nd)\n",
    "# print(type(y_nd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate into values and labels\n",
    "# ids, train = np.split(dataset_ndarr, [1], axis=1)\n",
    "# values, labels = np.split(train, [-1], axis=1)\n",
    "\n",
    "# flatten labels\n",
    "# labels_list = labels.flatten().tolist()\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, Normalizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# one hot encoder for categoricalize\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder\n",
    "# https://stackoverflow.com/questions/43588679/issue-with-onehotencoder-for-categorical-features\n",
    "ct = ColumnTransformer(\n",
    "    [('enc', OneHotEncoder(), [1])],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# normalizer\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html#sklearn.preprocessing.Normalizer\n",
    "norm = Normalizer()\n",
    "\n",
    "# build the preprocessing pipeline\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "prep = Pipeline(\n",
    "    [('ct', ct), \n",
    "     ('norm', norm)]\n",
    ")\n",
    "\n",
    "# fit the preprocessing pipeline\n",
    "prep.fit(X_nd)\n",
    "\n",
    "# split into train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_nd, y_nd, test_size=0.15)\n",
    "\n",
    "# print(X_train)\n",
    "# print(len(X_train))\n",
    "# print(len(X_train[0]))\n",
    "# print()\n",
    "\n",
    "# print(X_test)\n",
    "# print(len(X_test))\n",
    "# print()\n",
    "\n",
    "# print(y_train)\n",
    "# print(len(y_train))\n",
    "# print()\n",
    "\n",
    "# print(y_test)\n",
    "# print(len(y_test))\n",
    "# print()\n",
    "\n",
    "# transform on X_train and X_test\n",
    "X_train_pp = prep.transform(X_train)\n",
    "X_test_pp = prep.transform(X_test)\n",
    "\n",
    "# print(\"pre-processed train X: \")\n",
    "# print(X_train_pp)\n",
    "# print(len(X_train_pp))\n",
    "# print(len(X_train_pp[0]))\n",
    "# print()\n",
    "\n",
    "# convert y to list\n",
    "y_train_list = list(y_train)\n",
    "y_test_list = list(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "{'max_depth': 1, 'max_leaf_nodes': 4, 'min_samples_leaf': 1, 'min_samples_split': 4}\n",
      "[0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0]\n",
      "<class 'list'>\n",
      "[0.10769231 0.86956522 0.10769231 0.42307692 0.6875     0.86956522\n",
      " 0.10769231 0.10769231 0.10769231 0.10769231 0.86956522 0.10769231\n",
      " 0.10769231 0.10769231 0.10769231 0.10769231 0.86956522 0.10769231\n",
      " 0.42307692 0.10769231 0.6875     0.86956522 0.10769231 0.42307692\n",
      " 0.86956522 0.86956522 0.6875    ]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5470294503877641"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# regressors\n",
    "from sklearn.tree import DecisionTreeRegressor as DT\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor as NN\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "models = {\n",
    "    'DT': DT(),\n",
    "    'NN': NN(),\n",
    "    'LR': LR()\n",
    "}\n",
    "param_dict = {\n",
    "    'DT': {\n",
    "        'max_depth': [1,2,3,None],\n",
    "        'max_leaf_nodes': [4,6,8,10,None],\n",
    "        'min_samples_leaf': [1,2,3],\n",
    "        'min_samples_split': [2,4,6]\n",
    "    },\n",
    "    'NN': {\n",
    "        'hidden_layer_sizes': [1,3,5],\n",
    "        'activation': ['logistic','tanh','relu'],\n",
    "        'max_iter': [500,1000]\n",
    "    },\n",
    "    'LR': {\n",
    "        'C': [0.0001, 0.001, 0.01, 1, 10],\n",
    "        'penalty': ['l1','l2']\n",
    "    }\n",
    "}\n",
    "\n",
    "# grid search cross validation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "model_name = 'DT'\n",
    "model = models[model_name]\n",
    "scorer = 'neg_mean_squared_log_error'\n",
    "gscv = GridSearchCV(model, param_dict[model_name], cv=3, scoring=None)\n",
    "gscv.fit(X_train_pp, y_train_list)\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print(gscv.best_params_)\n",
    "\n",
    "# test on test data\n",
    "y_true, y_pred = y_test_list, gscv.predict(X_test_pp)\n",
    "\n",
    "print(y_true)\n",
    "print(type(y_true))\n",
    "print(y_pred)\n",
    "print(type(y_pred))\n",
    "\n",
    "# get metrics (log loss)\n",
    "log_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.00266674 ... 0.15733779 0.424012   0.        ]\n",
      " [0.         0.00339223 0.         ... 0.11872803 0.61738575 0.        ]\n",
      " [0.         0.         0.00401573 ... 0.17267649 0.48188788 0.00401573]\n",
      " ...\n",
      " [0.         0.00295011 0.         ... 0.12390478 0.52512026 0.        ]\n",
      " [0.         0.00311077 0.         ... 0.14309541 0.47283701 0.00311077]\n",
      " [0.         0.00295372 0.         ... 0.12700996 0.50508612 0.        ]]\n",
      "\n",
      "[0.86956522 0.10769231 0.86956522 0.10769231 0.6875     0.10769231\n",
      " 0.10769231 0.86956522 0.10769231 0.10769231 0.10769231 0.86956522\n",
      " 0.10769231 0.86956522 0.10769231 0.10769231 0.10769231 0.10769231\n",
      " 0.86956522 0.10769231 0.86956522 0.10769231 0.6875     0.10769231\n",
      " 0.42307692 0.86956522 0.86956522 0.10769231 0.6875     0.10769231\n",
      " 0.86956522 0.86956522 0.42307692 0.10769231 0.10769231 0.10769231\n",
      " 0.10769231 0.10769231 0.42307692 0.10769231 0.86956522 0.86956522\n",
      " 0.86956522 0.10769231 0.42307692 0.10769231 0.10769231 0.10769231\n",
      " 0.10769231 0.6875     0.10769231 0.10769231 0.86956522 0.10769231\n",
      " 0.42307692 0.10769231 0.86956522 0.10769231 0.10769231 0.86956522\n",
      " 0.10769231 0.86956522 0.10769231 0.6875     0.10769231 0.10769231\n",
      " 0.10769231 0.42307692 0.86956522 0.6875     0.10769231 0.86956522\n",
      " 0.86956522 0.86956522 0.86956522 0.86956522 0.86956522 0.86956522\n",
      " 0.10769231 0.86956522 0.86956522 0.6875     0.10769231 0.42307692\n",
      " 0.10769231 0.10769231 0.86956522 0.6875     0.10769231 0.10769231]\n"
     ]
    }
   ],
   "source": [
    "# predict on test data\n",
    "test_values_url = \"./dataset/Warm_Up_Machine_Learning_with_a_Heart_-_Test_Values.csv\"\n",
    "test_df = pd.read_csv(test_values_url)\n",
    "testset_ndarr = test_df.values\n",
    "test_ids, test_values = np.split(testset_ndarr, [1], axis=1)\n",
    "\n",
    "# test model\n",
    "processed_test_values = prep.transform(test_values)\n",
    "\n",
    "print(processed_test_values)\n",
    "print()\n",
    "\n",
    "test_pred = gscv.predict(processed_test_values)\n",
    "\n",
    "print(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to csv\n",
    "header = labels_df.columns\n",
    "col_id_name = header[0]\n",
    "col_label_name = header[1]\n",
    "\n",
    "test_ids_list = list(test_ids.flatten())\n",
    "\n",
    "res_dict = {col_id_name: test_ids_list,\n",
    "            col_label_name: test_pred\n",
    "           }\n",
    "res_df = pd.DataFrame.from_dict(res_dict)\n",
    "\n",
    "import time\n",
    "# export to df\n",
    "millis = int(round(time.time() * 1000))\n",
    "output_path = \"./\" + str(millis) + \".csv\"\n",
    "res_df.to_csv(output_path, index=False)\n",
    "\n",
    "# print(millis)\n",
    "# print(type(millis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
