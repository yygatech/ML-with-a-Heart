{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# set url of dataset\n",
    "train_values_url = \"./dataset/Warm_Up_Machine_Learning_with_a_Heart_-_Train_Values.csv\"\n",
    "train_labels_url = \"./dataset/Warm_Up_Machine_Learning_with_a_Heart_-_Train_Labels.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "values_df = pd.read_csv(train_values_url)\n",
    "labels_df = pd.read_csv(train_labels_url)\n",
    "\n",
    "# concatenate values and labels\n",
    "# https://pandas.pydata.org/pandas-docs/stable/merging.html\n",
    "# dataset_df = pd.merge(values_df, labels_df, on=['patient_id', 'patient_id'])\n",
    "\n",
    "# convert to nd-array\n",
    "# dataset_nd = dataset_df.values\n",
    "\n",
    "# print(dataset_nd)\n",
    "# print(len(dataset_nd))\n",
    "# print(len(dataset_nd[0]))\n",
    "\n",
    "values_nd = values_df.values\n",
    "labels_nd = labels_df.values\n",
    "\n",
    "# optional\n",
    "# drop null value\n",
    "# https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html\n",
    "# dataset_df.dropna()\n",
    "# Actually, nothing was dropped here.\n",
    "\n",
    "# dataset_ndarr = dataset_df.values\n",
    "ids_nd = values_nd[:, 0]\n",
    "X_nd = values_nd[:, 1:]\n",
    "# ids_nd, X_nd = np.split(values_df.values, [1], axis=1)\n",
    "y_nd = labels_nd[:, -1]\n",
    "\n",
    "# print(ids_nd)\n",
    "# print(type(ids_nd))\n",
    "\n",
    "# print(X_nd)\n",
    "# print(type(X_nd))\n",
    "\n",
    "# print(y_nd)\n",
    "# print(type(y_nd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate into values and labels\n",
    "# ids, train = np.split(dataset_ndarr, [1], axis=1)\n",
    "# values, labels = np.split(train, [-1], axis=1)\n",
    "\n",
    "# flatten labels\n",
    "# labels_list = labels.flatten().tolist()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, Normalizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# one hot encoder for categoricalize\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder\n",
    "# https://stackoverflow.com/questions/43588679/issue-with-onehotencoder-for-categorical-features\n",
    "ct = ColumnTransformer(\n",
    "    [('enc', OneHotEncoder(), [1])],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# normalizer\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html#sklearn.preprocessing.Normalizer\n",
    "norm = Normalizer()\n",
    "\n",
    "# build the preprocessing pipeline\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "prep = Pipeline(\n",
    "    [('ct', ct), \n",
    "     ('norm', norm)]\n",
    ")\n",
    "\n",
    "# fit the preprocessing pipeline\n",
    "prep.fit(X_nd)\n",
    "\n",
    "# split into train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_nd, y_nd, test_size=0.15)\n",
    "\n",
    "# print(X_train)\n",
    "# print(len(X_train))\n",
    "# print(len(X_train[0]))\n",
    "# print()\n",
    "\n",
    "# print(X_test)\n",
    "# print(len(X_test))\n",
    "# print()\n",
    "\n",
    "# print(y_train)\n",
    "# print(len(y_train))\n",
    "# print()\n",
    "\n",
    "# print(y_test)\n",
    "# print(len(y_test))\n",
    "# print()\n",
    "\n",
    "# transform on X_train and X_test\n",
    "X_train_pp = prep.transform(X_train)\n",
    "X_test_pp = prep.transform(X_test)\n",
    "\n",
    "# print(\"pre-processed train X: \")\n",
    "# print(X_train_pp)\n",
    "# print(len(X_train_pp))\n",
    "# print(len(X_train_pp[0]))\n",
    "# print()\n",
    "\n",
    "# convert y to list\n",
    "y_train_list = list(y_train)\n",
    "y_test_list = list(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/henry/miniconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n{'activation': 'tanh', 'hidden_layer_sizes': (100, 100, 50), 'max_iter': 1000}\nLog Loss: 0.28924956662392204\n"
     ]
    }
   ],
   "source": [
    "# regressors\n",
    "from sklearn.tree import DecisionTreeRegressor as DT\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor\n",
    "from sklearn.neural_network import MLPRegressor as NN\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier as MLP\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB as NB\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.ensemble import BaggingClassifier as Bagging\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "from sklearn.ensemble import AdaBoostClassifier as AdaBoost\n",
    "from sklearn.ensemble import GradientBoostingClassifier as GB\n",
    "from xgboost import XGBClassifier as XGB\n",
    "# conda install -c conda-forge xgboost\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "\n",
    "models = {\n",
    "    'DT': DT(),\n",
    "    'NN': NN(),\n",
    "    'LR': LR(),\n",
    "    'MLP': MLP(),\n",
    "    'SVC': SVC(),\n",
    "    'NB': NB(),\n",
    "    'KNN': KNN(),\n",
    "    'Bagging': Bagging(),\n",
    "    'RF': RF(),\n",
    "    'AdaBoost': AdaBoost(),\n",
    "    'GB': GB(),\n",
    "    'XGB': XGB(),\n",
    "}\n",
    "param_dict = {\n",
    "    # 0.67 {'max_depth': 1, 'max_leaf_nodes': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
    "    'DT': {\n",
    "        'max_depth': [1,2,3,None],\n",
    "        'max_leaf_nodes': [4,6,8,10,None],\n",
    "        'min_samples_leaf': [1,2,3],\n",
    "        'min_samples_split': [2,4,6]\n",
    "    },\n",
    "    # 0.40 {'activation': 'tanh', 'hidden_layer_sizes': (100, 100, 50), 'max_iter': 2000}\n",
    "    'NN': {\n",
    "        'hidden_layer_sizes': [(100,100,50)],\n",
    "        'activation': ['relu', 'tanh'],\n",
    "        'max_iter': [500, 1000, 2000]\n",
    "    },\n",
    "    # 12.79 {'C': 5.0, 'class_weight': 'balanced', 'fit_intercept': True, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
    "    'LR': {\n",
    "        \"solver\": ['lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "        \"penalty\": ['l2'],\n",
    "        \"C\": [1.0, 1.5, 2.0, 5.0],\n",
    "        \"fit_intercept\": [True, False],\n",
    "        \"class_weight\": [None, 'balanced']\n",
    "    },\n",
    "    # 10.23 {'activation': 'relu', 'early_stopping': False, 'hidden_layer_sizes': 5, 'learning_rate': 'adaptive', 'max_iter': 1000}\n",
    "    'MLP': {\n",
    "        \"max_iter\": [1000, 2000],\n",
    "       \"hidden_layer_sizes\": [5],\n",
    "       \"activation\": ['tanh', 'relu'],\n",
    "       \"learning_rate\": ['constant', 'invscaling', 'adaptive'],\n",
    "       \"early_stopping\": [True, False],\n",
    "    },\n",
    "    # 8.95 {'C': 10000, 'coef0': 0.0, 'gamma': 'scale', 'kernel': 'sigmoid', 'shrinking': True}\n",
    "    'SVC': {\n",
    "        \"C\": [5000, 10000, 20000, 30000],\n",
    "        \"kernel\": [\"poly\", \"rbf\", \"sigmoid\"],\n",
    "        \"coef0\": [0.0, 0.1, 0.2, 0.3, 0.5],\n",
    "        \"shrinking\": [True, False],\n",
    "        \"gamma\": ['scale', 'auto']\n",
    "    },\n",
    "    # 6.39 {}\n",
    "    'NB': {\n",
    "        # Nothing can be tuned\n",
    "    },\n",
    "    # 11.51 {'algorithm': 'auto', 'n_neighbors': 3, 'p': 1, 'weights': 'distance'}\n",
    "    'KNN': {\n",
    "        \"n_neighbors\": [1, 2, 3, 5, 7],\n",
    "        \"weights\": ['uniform', 'distance'],\n",
    "        \"algorithm\": ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "        \"p\": [1, 2, 3]\n",
    "    },\n",
    "    # 8.95 {'max_features': 1.0, 'max_samples': 0.1, 'n_estimators': 50}\n",
    "    'Bagging': {\n",
    "          \"n_estimators\": [10, 20, 50, 100],\n",
    "          \"max_samples\": [0.01, 0.1, 0.3],\n",
    "          \"max_features\": [0.5, 0.8, 1.0]\n",
    "    },\n",
    "    # 7.67 {'criterion': 'entropy', 'max_depth': 20, 'max_features': 'auto', 'n_estimators': 20}\n",
    "    'RF': {\n",
    "        \"n_estimators\": [10, 20, 50],\n",
    "        \"criterion\": [\"gini\", \"entropy\"],\n",
    "        \"max_depth\": [None, 5, 10, 20],\n",
    "        \"max_features\": [None, \"auto\", \"log2\"]\n",
    "    },\n",
    "    # 7.67 {'learning_rate': 0.5, 'max_depth': 10, 'max_features': 'log2', 'n_estimators': 100}\n",
    "    'GB': {\n",
    "          \"learning_rate\": [0.05, 0.1, 0.2, 0.5, 1.0],\n",
    "          \"n_estimators\": [10, 50, 100, 200],\n",
    "          \"max_depth\": [5, 10],\n",
    "          \"max_features\": [None, \"auto\", \"log2\"]\n",
    "    },\n",
    "    # 7.67\n",
    "    'AdaBoost': {\n",
    "        \"n_estimators\": [20, 50, 100, 1000],\n",
    "        \"learning_rate\": [0.01, 0.1, 1],\n",
    "        \"algorithm\": [\"SAMME\", \"SAMME.R\"],\n",
    "    },\n",
    "    # 8.9 {'booster': 'gbtree', 'learning_rate': 0.1, 'min_child_weight': 5, 'n_estimators': 100}\n",
    "    'XGB': {\n",
    "        \"learning_rate\": [0.01, 0.1],\n",
    "        \"n_estimators\": [5, 10, 20, 100],\n",
    "        \"min_child_weight\": [3, 5, 10],\n",
    "        \"booster\": ['gbtree', 'gblinear', 'dart']\n",
    "    }\n",
    "}\n",
    "\n",
    "# grid search cross validation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "model_name = 'NN'\n",
    "model = models[model_name]\n",
    "scorer = 'neg_mean_squared_log_error'\n",
    "gscv = GridSearchCV(model, param_dict[model_name], cv=5, scoring=None)\n",
    "gscv.fit(X_train_pp, y_train_list)\n",
    "\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print(gscv.best_params_)\n",
    "\n",
    "# test on test data\n",
    "y_true, y_pred = y_test_list, gscv.predict(X_test_pp)\n",
    "\n",
    "# print(y_true)\n",
    "# print(type(y_true))\n",
    "# print(y_pred)\n",
    "# print(type(y_pred))\n",
    "\n",
    "# get metrics (log loss)\n",
    "print(\"Log Loss:\", log_loss(y_true, y_pred))\n",
    "# print(\"Accuracy Score:\", accuracy_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.00266674 ... 0.15733779 0.424012   0.        ]\n [0.         0.00339223 0.         ... 0.11872803 0.61738575 0.        ]\n [0.         0.         0.00401573 ... 0.17267649 0.48188788 0.00401573]\n ...\n [0.         0.00295011 0.         ... 0.12390478 0.52512026 0.        ]\n [0.         0.00311077 0.         ... 0.14309541 0.47283701 0.00311077]\n [0.         0.00295372 0.         ... 0.12700996 0.50508612 0.        ]]\n\n[ 0.38815372  0.19595794  1.19935958  0.10200492  0.99612034  0.08124309\n  0.25992542  0.89852452  0.21095232  0.28180369  0.57392963  0.53887786\n  0.33316518  0.94602252  0.0328443   0.07228183 -0.09256281 -0.09747319\n  0.81914079  0.02722626  0.95006084  0.35167875  0.15234588 -0.13384188\n  0.39343903  1.04998085  0.43082913  0.22477232  0.47591777  0.04014866\n  0.87753542  0.30488802  0.5091827   0.5825555   0.21816872  0.10633718\n  0.28128012  0.3813328   0.13910868  0.07736251  0.85359593  0.04766525\n  0.90002043  0.06665682  0.88960983  0.21001659  0.17306169  0.2610935\n  0.3515832   0.77746213  0.8159868   0.22240038  1.15625888  0.1290569\n  0.41070518 -0.01039221  0.85870954  0.26812685  0.51011387  0.58852343\n  0.1038137   0.93475839  0.26471534  1.05249361  0.01432694  0.81404504\n  0.63730683  0.41605343  0.63921385  0.71876781  0.12098969  1.12065555\n  0.95731475  1.11662864  0.98417253  1.00808861  0.9364812   0.61637758\n  0.23057659  0.59958781  0.43353889 -0.03056529  0.48667055  0.62937848\n  0.40479589  0.04698088  0.74654532  0.38280703  0.42796603  0.35074081]\n"
     ]
    }
   ],
   "source": [
    "# predict on test data\n",
    "test_values_url = \"./dataset/Warm_Up_Machine_Learning_with_a_Heart_-_Test_Values.csv\"\n",
    "test_df = pd.read_csv(test_values_url)\n",
    "testset_ndarr = test_df.values\n",
    "test_ids, test_values = np.split(testset_ndarr, [1], axis=1)\n",
    "\n",
    "# test model\n",
    "processed_test_values = prep.transform(test_values)\n",
    "\n",
    "print(processed_test_values)\n",
    "print()\n",
    "\n",
    "test_pred = gscv.predict(processed_test_values)\n",
    "\n",
    "print(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to csv\n",
    "header = labels_df.columns\n",
    "col_id_name = header[0]\n",
    "col_label_name = header[1]\n",
    "\n",
    "test_ids_list = list(test_ids.flatten())\n",
    "\n",
    "res_dict = {col_id_name: test_ids_list,\n",
    "            col_label_name: test_pred\n",
    "           }\n",
    "res_df = pd.DataFrame.from_dict(res_dict)\n",
    "\n",
    "import time\n",
    "# export to df\n",
    "millis = int(round(time.time() * 1000))\n",
    "output_path = \"./\" + str(millis) + \".csv\"\n",
    "res_df.to_csv(output_path, index=False)\n",
    "\n",
    "# print(millis)\n",
    "# print(type(millis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
